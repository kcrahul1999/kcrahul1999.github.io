---
title: "Poisson Regression Examples"
author: "Rahul Kc"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor_options: 
  chunk_output_type: console
---

## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.

:::: {.callout-note collapse="true"}
### Data

_todo: Read in data._

_todo: Compare histograms and means of number of patents by customer status. What do you observe?_
::::

```{python}
#| message: false
#| echo: false
pip install pandas scipy statsmodels pyyaml nbformat nbclient matplotlib numpy -q
```

```{python}
#Import libarary and data file
import pandas as pd
from scipy.stats import ttest_ind
import statsmodels.api as sm
import matplotlib.pyplot as plt
import numpy as np
import math  
import scipy.optimize as optimize
import scipy.stats as stats
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

air = pd.read_csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/airbnb.csv")
blue = pd.read_csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/blueprinty.csv")
```

```{python}
blue.head()
```

```{python}
blue.describe()
```

First I compare histograms and means of number of patents by customer status. I observe that the mean of non customers (iscustomer=0) is 3.62 and the mean of customers (iscustomer=1) is 4.09.

This means firms using the software have a higher average number of patents to firms that don't. 

```{python}
#Calculate the mean number of patents for customers and non-customers
means = blue.groupby('iscustomer')['patents'].mean()

#Create histograms to compare the distribution of patents
plt.figure(figsize=(10, 6))
blue[blue['iscustomer'] == 1]['patents'].hist(alpha=0.5, label='Customers', bins=20)
blue[blue['iscustomer'] == 0]['patents'].hist(alpha=0.5, label='Non-Customers', bins=20)
plt.title('Histogram of Number of Patents by Customer Status')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.legend()
plt.show()

print(means)
```

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

Therefore I compared regions and ages by customer status.

:::: {.callout-note collapse="true"}
#### TODO
_todo: Compare regions and ages by customer status. What do you observe?_
::::

We see below that the number of non-customers is far greater than the number of customers for all regions. The '1' representing customers and '0' representing non-customers.

```{python}
#Accounting missing data for cols 'region' and 'age'
blue['region'] = blue['region'].fillna('Unknown')
blue['age'] = blue['age'].fillna(blue['age'].mean())

#Compare regions by customer status
region_summary = blue.groupby(['region', 'iscustomer']).size().unstack(fill_value=0)

#Plott regional distribution by customer status
region_summary.plot(kind='bar', figsize=(14, 7), stacked=True)
plt.title('Distribution of Firms by Region and Customer Status')
plt.xlabel('Region')
plt.ylabel('Number of Firms')
plt.legend(title='Customer Status', labels=['Non-Customers', 'Customers'])
plt.xticks(rotation=45)
plt.show()

#Group by 'region' and 'iscustomer' and get count of firms
region_customer_counts = blue.groupby(['region', 'iscustomer']).size().unstack(fill_value=0)

print(region_customer_counts)
```

Below we see that the mean age of non-customers (iscustomer=0) is 26.69 years, while the mean age of customers (iscustomer=1) is 24.15 years.

This means that on avg. the firms using the software about 2.5 years younger. 

```{python}
import matplotlib.pyplot as plt

#fig size
plt.figure(figsize=(14, 7))

#Create histogram for non-customers 
blue[blue['iscustomer'] == 0]['age'].hist(alpha=0.5, label='Non-Customers', bins=20, edgecolor='black')

#Create histogram for customers
blue[blue['iscustomer'] == 1]['age'].hist(alpha=0.5, label='Customers', bins=20, edgecolor='black')

#Add title, labels, and legend
plt.title('Distribution of Firm Age by Customer Status')
plt.xlabel('Age Since Incorporation')
plt.ylabel('Number of Firms')
plt.legend()

plt.show()

#Calculate the mean age for each group 
age_means = blue.groupby('iscustomer')['age'].mean()
print("Mean Age by Customer Status:")
print(age_means)
```



### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

:::: {.callout-note collapse="true"}
#### TODO
_todo: Write down mathematically the likelihood for_ $Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.

_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_
:::: 

The function below calulates the possion likelihood of Y given the lambda. Lamda is the parameter of the posson distribution which must be greater than 0. Y is the array of variable we're counting. The function itself will return the likelihood of Y given lambda.

```{python}
def poisson_likelihood(lambda_, Y):
    if lambda_ <= 0:
        raise ValueError("Lambda must be greater than 0")
    
    # Calculate the Poisson probability for each Y_i
    probabilities = (np.exp(-lambda_) * lambda_**Y) / np.array([np.math.factorial(y) for y in Y])

    #Product of probabilities
    return np.prod(probabilities)

#inputs
Y = np.array([1, 0, 1, 3, 4, 2, 0])
lambda_ = 1.9

#likelihood
likelihood = poisson_likelihood(lambda_, Y)
print("Poisson Likelihood:", likelihood)
```

:::: {.callout-note collapse="true"}
#### TODO
_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._

_todo: If you're feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which "feels right" because the mean of a Poisson distribution is lambda._

_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._
::::

Using the function to plot lambda on the horizontal axis and the log likelihood on the vertical axis for a range of lambdas using the observed number of patents as the input for Y gets the following.

```{python}
import numpy as np
import matplotlib.pyplot as plt
import math

def poisson_loglikelihood(lambda_, Y):
    if lambda_ <= 0:
        return -np.inf  # Return negative infinity for non-positive lambda values to avoid computational errors
    log_likelihood = -len(Y) * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum([math.log(math.factorial(y)) for y in Y])
    return log_likelihood

#Set y to patent count 
Y = blue['patents'].values  

#Make a range of lambda from 0.1 to 10, with 100 points
lambda_values = np.linspace(0.1, 10, 100)
log_likelihood_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]

#Plot
plt.figure(figsize=(10, 6))
plt.plot(lambda_values, log_likelihood_values, label='Poisson Log-Likelihood')
plt.title('Poisson Log-Likelihood vs. Lambda for Observed Patent Counts')
plt.xlabel('Lambda')
plt.ylabel('Log-Likelihood')
plt.grid(True)
plt.legend()
plt.show()
```

To Find the MLE I optimizied the likelihood functionsp.optimize().

```{python}
result = optimize.minimize(
    fun=poisson_loglikelihood,
    x0=[1.0],  
    args=(Y,),  
    bounds=[(0.001, None)] 
)

print("MLE for lambda:", result.x)
print("Optimization success:", result.success)
print("Log-Likelihood at MLE:", -result.fun)
```

We see that the MLE = 0.001. This could be because of the initial guess, or constraints set in the optimization, or because its a log likelihood function.

To solve this I did the following 
- The initial guess was changed to the mean of the observed data 
- The function was modified to return the negative log-likelihood because scipy.optimize is designed to minimize functions. Therefore a negative will to maximize the original log-likelihood.

```{python}
def poisson_loglikelihood_neg(lambda_, Y):
    if lambda_ <= 0:
        return np.inf  
    return -(-len(Y) * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum([math.log(math.factorial(y)) for y in Y]))

Y = blue['patents'].values

#get MLE with scipy.optimize.minimize
result = optimize.minimize(
    fun=poisson_loglikelihood_neg,  
    x0=[np.mean(Y)],  
    args=(Y,), 
    bounds=[(0.001, None)], 
    options={'disp': True}  
)

print("MLE for lambda:", result.x)
print("Optimization success:", result.success)
print("Log-Likelihood at MLE:", -result.fun)
```


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

:::: {.callout-note collapse="true"}
#### TODO
_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that_ $\lambda_i = e^{X_i'\beta}$. _For example:_
::::

```{python}
def poisson_regression_loglikelihood(beta, X, Y):
    #Set linear combination with inputs and parameters
    linear_combination = X @ beta
    
    #Get lambda 
    lambda_i = np.exp(linear_combination)
    
    #Log-likelihood
    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - np.array([np.math.factorial(y) for y in Y]))
    
    return log_likelihood 
```

:::: {.callout-note collapse="true"}
#### TODO
_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._

_todo: Check your results using R's glm() function or Python sm.GLM() function._

_todo: Interpret the results. What do you conclude about the effect of Blueprinty's software on patent success?_
::::

First i used the function with sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Using the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.

---------------work--------------------------------

```{python}
import numpy as np
import pandas as pd
import scipy.optimize as optimize
import scipy.special as sps

def poisson_regression_loglikelihood_neg(beta, X, Y):
    eta = np.dot(X, beta)  
    lambda_i = np.exp(eta.clip(-10, 10)) 
    log_likelihood = -np.sum(lambda_i - Y * np.log(lambda_i) + sps.gammaln(Y + 1))
    return log_likelihood

#Example setup of data
np.random.seed(0)
n = 100  # number of observations
X = np.random.normal(0, 1, (n, 3))  # example feature matrix with 3 features
X[:, 0] = 1  # replace first column with 1s for the intercept
beta_true = np.array([0.5, -0.2, 0.1])  # true beta coefficients for simulation
Y = np.random.poisson(lam=np.exp(np.dot(X, beta_true)))  # simulate response variable

#Initial guess for beta parameters
beta_init = np.zeros(X.shape[1])

#Optimize to find MLE of beta using BFGS method
result = optimize.minimize(
    fun=poisson_regression_loglikelihood_neg,
    x0=beta_init,
    args=(X, Y),
    method='BFGS'
)

#Extract the standard errors 
std_errors = np.sqrt(np.diag(result.hess_inv))

#Results
coefficients = result.x
coeff_std_errors = np.vstack((coefficients, std_errors)).T
coeff_table = pd.DataFrame(coeff_std_errors, columns=['Coefficient', 'Std Error'],
                           index=['Intercept', 'Feature1', 'Feature2'])

print("Optimization Success:", result.success)
print("Estimated Beta Coefficients:", result.x)
print("Standard Errors:", std_errors)
print("Coefficient Table:\n", coeff_table)
```

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not
::::

:::: {.callout-note collapse="true"}
#### TODO
_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._
::::

```{python}
air = pd.read_csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/airbnb.csv")
```

### Exploratory Data Analysis

```{python}
print(air.head())
print(air.isnull().sum())
print(air.describe())
```

```{python}
#Convert date columns to datetime
air['last_scraped'] = pd.to_datetime(air['last_scraped'])
air['host_since'] = pd.to_datetime(air['host_since'])
```

```{python}
#Delete issing values and create a new DataFrame 
air2 = air.copy()
air2['bathrooms'].fillna(air['bathrooms'].median(), inplace=True)
air2['bedrooms'].fillna(air['bedrooms'].median(), inplace=True)
air2['review_scores_cleanliness'].fillna(air['review_scores_cleanliness'].mean(), inplace=True)
air2['review_scores_location'].fillna(air['review_scores_location'].mean(), inplace=True)
air2['review_scores_value'].fillna(air['review_scores_value'].mean(), inplace=True)

#Remove rows where with null 
air2 = air2[~air2['host_since'].isnull()]

non_val_air2 = air2.isnull().sum()

non_val_air2
```

#### Regression Model
```{python}
#Rename columns to remove spaces
air2.rename(columns={
    'room_type_Private room': 'room_type_Private_room',
    'room_type_Shared room': 'room_type_Shared_room'
}, inplace=True)
```

```{python}
print(air2.columns)
```

```{python}
#Rename columns to replace spaces with underscores
air2.columns = [col.replace(' ', '_') for col in air2.columns]

#Create the model formula based on existing dummy variables
variables = ['price', 'days', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value']
room_vars = [col for col in air2.columns if 'room_type_' in col]  
formula = 'number_of_reviews ~ ' + ' + '.join(variables + room_vars)

#Model
model = smf.glm(formula=formula, data=air2, family=sm.families.Poisson()).fit()

print(model.summary())
```

#### Coefficients 
1. Intercept (3.0353):
Shows the log of the expected count of reviews when all other variables are zero.

2. Room Type:
Private Room (coef = -0.0822): Private rooms have about 8.22% fewer reviews than entire homes/apartments, holding other factors constant. 

Shared Room (coef = -0.2291): Shared rooms have about 22.91% fewer reviews compared to entire homes/apartments.

3. Price (coef = -0.0004):
Each unit increase in price is means a 0.04% decrease in the number of reviews.

4. Days (coef = 0.0006):
Each additional day a listing is posted means a 0.06% increase in the number of reviews.

5. Review Scores:
Cleanliness (coef = 0.1422): A one-point increase in the cleanliness score means 14.22% more reviews.

Location (coef = -0.1142): A higher location score is associated with fewer reviews. Price could be a factor associated with location

Value (coef = -0.1204): Higher value scores are similarly associated with fewer reviews.

