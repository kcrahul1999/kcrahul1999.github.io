---
title: "Key Drivers Analysis"
author: "Rahul Kc"
date: today
---

This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.

_todo: replicate the table on slide 19 of the session 4 slides. This involves calculating pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into Python._

_If you want a challenge, either (1) implement one or more of the measures yourself. "Usefulness" is rather easy to program up. Shapley values for linear regression are a bit more work. Or (2) add additional measures to the table such as the importance scores from XGBoost._

Pearson Correlations = Pearson Correlations

Polychoric Correlations = Standardized Coefficients

Standardized Multiple Regression Coefficients 

LMG / Shapley Values = Usefullness (R2)

Johnson's Epilon = Permutation Importance

Mean Decrease in RF Gini Coefficient


```{python}
pip install shap
```

```{python}
pip install polychoric
```

```{r}
install.packages("psych")
```

:::: {.callout-note collapse="true"}
#### Loading Libs and Data
```{python}
#Import libarary and data file
import pandas as pd
from scipy.stats import ttest_ind
import statsmodels.api as sm
import matplotlib.pyplot as plt
import numpy as np
import math  
import scipy.optimize as optimize
import scipy.stats as stats
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.optimize import minimize
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
```

```{python}
drive = pd.read_csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/data_for_drivers_analysis.csv")
```
::::

:::: {.callout-note collapse="true"}
#### EDA
```{python}

```

```{python}
drive.head()
```

```{python}
drive.describe()
```

```{python}
drive.info()
```
::::


:::: {.callout-note collapse="true"}
#### Code


The first thing I did was calculate the pearson correlation for drive
```{python}
# Step 1: Pearson Correlations
pearson_corr = drive.corr()['satisfaction']

# Display the Pearson correlations
pearson_corr

```

Next I calculated the polychoric correlations. I initially tried using the polychoric package but I was not able to get that to work. Fortunately I found a package in R called 'psych' which does exactly that in a few lines of code.

However this meant I would have to write the resulting csv becuase the alternative method that does not use this process is way more complex. 

```{r}
library(psych)

# Read the data
data <- read.csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/data_for_drivers_analysis.csv")

# Select only the ordinal variables
ordinal_vars <- data[c("trust", "build", "differs", "easy", "appealing", "rewarding", "popular", "service", "impact")]

# Calculate the polychoric correlation matrix
polychoric_corr_matrix <- polychoric(ordinal_vars)$rho

# Save the polychoric correlation matrix to a CSV file
write.csv(polychoric_corr_matrix, "C:/Users/kcrah/Desktop/Quarto_stuff/files/polychoric_corr_matrix.csv", row.names = TRUE)
```


```{python}
# Step 2: Load Polychoric Correlations
polychoric_corr = pd.read_csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/polychoric_corr_matrix.csv", index_col=0)
polychoric_corr = polychoric_corr['satisfaction']

# Display the Polychoric correlations
polychoric_corr.head()
```


Next I standardized the multiple regression coefficients
```{python}
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler

# Extract the predictors and response variable
X = drive.drop(columns=['satisfaction', 'id'])
y = drive['satisfaction']

# Standardize the predictors
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Add a constant term for the intercept
X_scaled = sm.add_constant(X_scaled)

# Fit the multiple regression model
model = sm.OLS(y, X_scaled).fit()

# Extract standardized coefficients
standardized_coeffs = model.params[1:]  # Exclude the intercept

standardized_coeffs
```

Then I Calculated the R^2 values from individual regression models for each predictor

```{python}
# Calculate R² values from individual regression models for each predictor
usefulness = {}
for col in X.columns:
    model = sm.OLS(y, sm.add_constant(X[col])).fit()
    usefulness[col] = model.rsquared

usefulness

```


```{python}
import shap
from sklearn.linear_model import LinearRegression
from sklearn.inspection import permutation_importance

# Fit a linear regression model using sklearn
linear_regressor = LinearRegression()
linear_regressor.fit(X_scaled[:, 1:], y)  # Exclude the intercept

# Calculate permutation importance
perm_importance = permutation_importance(linear_regressor, X_scaled[:, 1:], y, n_repeats=10, random_state=42)
perm_importance_mean = perm_importance.importances_mean

perm_importance_mean

```

Next I calculated the mean decrease in gini coefficient from the random Forest model

```{python}
from sklearn.ensemble import RandomForestRegressor

# Fit a random forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)
importances = rf_model.feature_importances_

importances

```

Finally I compiled the results into the dataFrame
```{python}
# Compile the results into a DataFrame for easy viewing
results = pd.DataFrame({
    'Pearson Correlations': pearson_corr[X.columns] * 100,  # convert to percentages
    'Standardized Coefficients': standardized_coeffs * 100,  # convert to percentages
    'Usefulness (R²)': pd.Series(usefulness) * 100,  # convert to percentages
    'Permutation Importance': perm_importance_mean * 100,  # convert to percentages
    'Mean Decrease in Gini': importances * 100  # convert to percentages
}, index=X.columns)

# Display the results
results

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```
::::