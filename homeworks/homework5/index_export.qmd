---
title: "Segmentation Methods"
author: Rahul Kc
date: today
---

:::: {.callout-note collapse="true"}
## K-Means
_todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working. Test your algorithm on either the Iris or PalmerPenguins datasets. Compare your results to the built-in `kmeans` function in R or Python._

_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). How many clusters are suggested by these two metrics?_
::::

I started by importing the necessary libraries and reading in the data files.
:::: {.callout-note collapse="true"}
#### Import libs and data

```{python}
pip install yellowbrick
```

```{python}
#Import libarary and data file
import pandas as pd
from scipy.stats import ttest_ind
import statsmodels.api as sm
import matplotlib.pyplot as plt
import numpy as np
import math  
import scipy.optimize as optimize
import scipy.stats as stats
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.optimize import minimize
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from dominance_analysis import Dominance
from sklearn.linear_model import LinearRegression
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestRegressor
import shap
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer
```

```{python}
iris = pd.read_csv('C:/Users/kcrah/Desktop/Quarto_stuff/files/iris.csv')
```
::::

Then I did simple EDA to review the data and make sure nothing is missing.
:::: {.callout-note collapse="true"}
#### EDA
```{python}
print(iris.head())
```

```{python}
print(iris.describe())
```

```{python}
print(iris.info())
```
::::

#### Analysis

I started by standardizing the data so that the features contribute equally.
```{python}
scaler = StandardScaler()
iris_scale = scaler.fit_transform(iris.iloc[:, :-1])
```

Afterwords I created 4 seperate functions that for each cluster that will create centroids, assign the clusters, then one to update each centroids, and then one to plot.
```{python}
# Create centroids 
def create_centroids(data, k):
    indices = np.random.choice(data.shape[0], k, replace=False)
    return data[indices]

# Assign clusters 
def assign_clusters(data, centroids):
    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

# Update centroids 
def update_centroids(data, labels, k):
    return np.array([data[labels == i].mean(axis=0) for i in range(k)])

# Plot the Kmeans 
def plot_kmeans(data, centroids, labels, iteration):
    plt.figure(figsize=(10, 6))
    colors = ['r', 'g', 'b']
    for i in range(len(data)):
        plt.scatter(data[i, 0], data[i, 1], c=colors[int(labels[i])], s=30)

    for i in range(len(centroids)):
        plt.scatter(centroids[i, 0], centroids[i, 1], c=colors[i], marker='x', s=200, linewidths=3)
    plt.title(f'Iteration {iteration}')
    plt.show()
```

Next i created a function to actually run the Kmeans and then ploted each. In the same code block I calculated the last within-cluster-sum-of-squares wcss and silhouette score.
```{python}
def kmeans_with_plots(data, k, max_iters=100, tolerance=1e-4, plot_steps=True):
    centroids = create_centroids(data, k)
    if plot_steps:
        plot_kmeans(data, centroids, np.zeros(data.shape[0]), 0)
    
    for i in range(max_iters):
        labels = assign_clusters(data, centroids)
        
        # Plot the current step
        if plot_steps:
            plot_kmeans(data, centroids, labels, i+1)
        
        # Update centroids
        new_centroids = update_centroids(data, labels, k)
        
        # Check for convergence
        if np.linalg.norm(new_centroids - centroids) < tolerance:
            break
        centroids = new_centroids
    
    # Final plot
    if plot_steps:
        plot_kmeans(data, centroids, labels, i+1)
    
    # WCSS
    wcss = sum(np.sum((data[labels == i] - centroids[i])**2) for i in range(k))
    
    # Silhouette score
    silhouette_avg = silhouette_score(data, labels)
    
    return centroids, labels, wcss, silhouette_avg
```

This chunk runs the K-means algorithm on the standardized Iris dataset and prints the results.
```{python}
# Run the function on iris
centroids, labels, wcss, silhouette_avg = kmeans_with_plots(iris_scale, k=3)
```

```{python}
print("Custom Kmeans")
print("wcss:", wcss)
print("Silhouette Score:", silhouette_avg)
```

We see that for the Custom Kmeans wcss is 140.90 and the silhoutte score is 0.45.

Finally I run the Kmeans using the built in function.
```{python}
# Use built in KMeans from sklearn
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(iris_scale)

# Calculate wcss and silhouette score
bi_wcss = kmeans.inertia_
built_in_silhouette_avg = silhouette_score(iris_scale, kmeans.labels_)
```

```{python}
print("'Built in' KMeans")
print("wcss:", bi_wcss)
print("silhouette score:", built_in_silhouette_avg)
```

We see that for the 'Built in' Kmeans wcss is 140.90 and the silhoutte score is 0.45.

Then I made a dataframe which just pulls the outputs and puts into a dataframe for viewing.
```{python}
output = {
    "Metric": ["wcss", "silhouette score"],
    "Custom kmeans": [round(wcss, 2), round(silhouette_avg, 2)],
    "'Built in' KMeans": [round(bi_wcss, 2), round(built_in_silhouette_avg, 2)]
}

output_df = pd.DataFrame(output)
output_df
```

Next I calculated the optimal clusters for wcss using the 'elbow method' from the yellow road lib.
```{python}
# Elbow Method 
model = KMeans(random_state=42)
visualizer = KElbowVisualizer(model, k=(2,10), timings=False)

visualizer.fit(iris_scale)        
visualizer.show()                  

# Get optimal number of clusters
optimal_elbow = visualizer.elbow_value_
print(f"Optimal number of clusters (Elbow Method): {optimal_elbow}")
```

This function works by finding the point where wcss starts to decrease more slowly as the number of clusters increases. 

This means that 4 clusters, the reduction in wcss starts to plataeu means more clusters will not improve.

```{python}
# Calculate all the silhouette scores
silhouette_scores = {}
for k in range(2, 10):
    model = KMeans(n_clusters=k, random_state=42)
    labels = model.fit_predict(iris_scale)
    score = silhouette_score(iris_scale, labels)
    silhouette_scores[k] = score

# Find optimal clusters based on highest silhouette score
optimal_silhouette = max(silhouette_scores, key=silhouette_scores.get)
print(f"Optimal number of clusters (Silhouette Analysis): {optimal_silhouette}")
```

The silhouette score of 2, which measures the points between the clusters, and the highest is the optimal cluster.

Between the two methods on a visual level we see that the elbow method may be the better indication of the best cluster because we see very little improvement from the 4nd cluster onwards to the 7th, but there is improvement from the 2nd cluster to the 4th.