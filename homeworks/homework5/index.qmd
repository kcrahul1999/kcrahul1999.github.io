---
title: "Segmentation Methods"
author: Rahul Kc
date: today
---

:::: {.callout-note collapse="true"}
## K-Means
_todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working. Test your algorithm on either the Iris or PalmerPenguins datasets. Compare your results to the built-in `kmeans` function in R or Python._

_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). How many clusters are suggested by these two metrics?_
::::

I started by importing the necessary libraries and reading in the data files.
:::: {.callout-note collapse="true"}
#### Import libs and data

```{python}
pip install yellowbrick
```

```{python}
#Import libarary and data file
import pandas as pd
from scipy.stats import ttest_ind
import statsmodels.api as sm
import matplotlib.pyplot as plt
import numpy as np
import math  
import scipy.optimize as optimize
import scipy.stats as stats
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.optimize import minimize
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from dominance_analysis import Dominance
from sklearn.linear_model import LinearRegression
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestRegressor
import shap
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer
```

```{python}
iris = pd.read_csv('C:/Users/kcrah/Desktop/Quarto_stuff/files/iris.csv')
```
::::

Then I did simple EDA to review the data and make sure nothing is missing.
:::: {.callout-note collapse="true"}
#### EDA
```{python}
print(iris.head())
```

```{python}
print(iris.describe())
```

```{python}
print(iris.info())
```
::::

#### Analysis

I started by standardizing the data so that the features contribute equally.
```{python}
scaler = StandardScaler()
iris_scale = scaler.fit_transform(iris.iloc[:, :-1])
```

Afterwords I created 4 seperate functions that initializing centroids, assigning clusters, updating centroids, and plotting the K-means steps.
```{python}
# Create centroids 
def create_centroids(data, k):
    indices = np.random.choice(data.shape[0], k, replace=False)
    return data[indices]

# Assign clusters based on the closest centroids
def assign_clusters(data, centroids):
    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

# Update centroids to be the mean of points in each cluster
def update_centroids(data, labels, k):
    return np.array([data[labels == i].mean(axis=0) for i in range(k)])

# Plot the current state of the K-means clustering
def plot_kmeans(data, centroids, labels, iteration):
    plt.figure(figsize=(8, 6))
    colors = ['r', 'g', 'b']
    for i in range(len(data)):
        plt.scatter(data[i, 0], data[i, 1], c=colors[int(labels[i])], s=30)
    for i in range(len(centroids)):
        plt.scatter(centroids[i, 0], centroids[i, 1], c=colors[i], marker='x', s=200, linewidths=3)
    plt.title(f'Iteration {iteration}')
    plt.show()
```

This function runs the K-means algorithm, plotting the steps along the way and calculating the final within-cluster sum-of-squares and silhouette score.
```{python}
def kmeans_with_plots(data, k, max_iters=100, tolerance=1e-4, plot_steps=True):
    # Create centroids
    centroids = create_centroids(data, k)
    if plot_steps:
        plot_kmeans(data, centroids, np.zeros(data.shape[0]), 0)
    
    for i in range(max_iters):
        # Assign clusters based on closest centroids
        labels = assign_clusters(data, centroids)
        
        # Plot the current step
        if plot_steps:
            plot_kmeans(data, centroids, labels, i+1)
        
        # Update centroids
        new_centroids = update_centroids(data, labels, k)
        
        # Check for convergence
        if np.linalg.norm(new_centroids - centroids) < tolerance:
            break
        centroids = new_centroids
    
    # Final plot
    if plot_steps:
        plot_kmeans(data, centroids, labels, i+1)
    
    # Calculate within-cluster sum-of-squares
    wcss = sum(np.sum((data[labels == i] - centroids[i])**2) for i in range(k))
    
    # Calculate silhouette score
    silhouette_avg = silhouette_score(data, labels)
    
    return centroids, labels, wcss, silhouette_avg
```

This chunk runs the K-means algorithm on the standardized Iris dataset and prints the results.
```{python}
# Test the function with the Iris dataset
centroids, labels, wcss, silhouette_avg = kmeans_with_plots(iris_scale, k=3)
```

```{python}
print("Custom Kmeans")
print("Within cluster sum of squares:", wcss)
print("Silhouette Score:", silhouette_avg)
```

We see that for the Custom Kmeans within cluster sum of squares is 140.90 and the silhoutte score is 0.45.

Finally I run the Kmeans using the built in function.
```{python}
# Use built in KMeans from sklearn
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(iris_scale)

# Calculate within-cluster sum-of-squares and silhouette score
built_in_wcss = kmeans.inertia_
built_in_silhouette_avg = silhouette_score(iris_scale, kmeans.labels_)
```

```{python}
print("'Built in' KMeans")
print("within cluster sum of squares:", built_in_wcss)
print("silhouette score:", built_in_silhouette_avg)
```

We see that for the 'Built in' Kmeans within cluster sum of squares is 140.90 and the silhoutte score is 0.45.

Then I made a dataframe which just pulls the outputs and puts into a dataframe for viewing.
```{python}
results = {
    "Metric": ["within cluster sum of squares", "silhouette score"],
    "Custom kmeans": [round(wcss, 2), round(silhouette_avg, 2)],
    "'Built in' KMeans": [round(built_in_wcss, 2), round(built_in_silhouette_avg, 2)]
}

results_df = pd.DataFrame(results)
results_df
```

Next I calculated the optimal clusters for within-cluster  using the 'elbow method' from the yellow road lib.
```{python}
# Elbow Method 
model = KMeans(random_state=42)
visualizer = KElbowVisualizer(model, k=(2,10), timings=False)

visualizer.fit(iris_scale)        
visualizer.show()                  

# Get optimal number of clusters
optimal_clusters_elbow = visualizer.elbow_value_
print(f"Optimal number of clusters (Elbow Method): {optimal_clusters_elbow}")
```

This function works by finding the within cluster sum of squares starts to decrease more slowly as the number of clusters increases. 

This means that 4 clusters, the reduction in within cluster starts to plataeu means more clusters will not improve.

```{python}
# Calculate all the silhouette scores
silhouette_scores = {}
for k in range(2, 10):
    model = KMeans(n_clusters=k, random_state=42)
    labels = model.fit_predict(iris_scale)
    score = silhouette_score(iris_scale, labels)
    silhouette_scores[k] = score

# Find optimal number of clusters based on the highest silhouette score
optimal_clusters_silhouette = max(silhouette_scores, key=silhouette_scores.get)
print(f"Optimal number of clusters (Silhouette Analysis): {optimal_clusters_silhouette}")
```

The silhouette score of 2, measures the points between the clusters, and the highest is the optimal cluster.