---
title: "Multinomial Logit Examples"
author: "Rahul Kc"
date: today
---


This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans.


## 1. Estimating Yogurt Preferences

### Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 4 products, then either $y=3$ or $y=(0,0,1,0)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, size, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 4 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta} + e^{x_4'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=\delta_{i4}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 \times \mathbb{P}_i(4)^0 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$


### Yogurt Dataset

We will use the `yogurt_data` dataset, which provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc.

:::: {.callout-note collapse="true"}
#### Import Libs
```{python}
#Import libarary and data file
import pandas as pd
from scipy.stats import ttest_ind
import statsmodels.api as sm
import matplotlib.pyplot as plt
import numpy as np
import math  
import scipy.optimize as optimize
import scipy.stats as stats
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.optimize import minimize
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
```
::::

```{python}
yog = pd.read_csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/yogurt_data.csv")
```

```{python}
yog.head()
```

```{python}
yog.describe()
```


From the head and describe function we see that the data has transactions from consumers at a yogurt place. The yogurt place has 3 yogurt flavors.

:::: {.callout-note collapse="true"}
#### TODO: reshape and prep the data
Let the vector of product features include brand dummy variables for yogurts 1-3 (we'll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts' prices:  

$$ x_j' = [\mathbbm{1}(\text{Yogurt 1}), \mathbbm{1}(\text{Yogurt 2}), \mathbbm{1}(\text{Yogurt 3}), X_f, X_p] $$

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). 

What we would like to do is reorganize the data from a "wide" shape with $n$ rows and multiple columns for each covariate, to a "long" shape with $n \times J$ rows and a single column for each covariate.  As part of this re-organization, we'll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be "pivoted" or "melted" from wide to long.  
::::

I start by adding binary variables to indicate the first 3 products; and then I "pivoted" and "melted" from a wide dataframe to long dataframe for further data analysis. 

```{python}
#melt to long format
id_vars = ['id']
value_vars = ['f' + str(j) for j in range(1, 5)] + ['p' + str(j) for j in range(1, 5)] + ['y' + str(j) for j in range(1, 5)]
yogl = pd.melt(yog, id_vars=id_vars, value_vars=value_vars, var_name='variable', value_name='value')

#split col variable into cols type and product 
yogl['product'] = yogl['variable'].str.extract(r'(\d+)').astype(int)
yogl['type'] = yogl['variable'].str.extract(r'([a-z]+)')

#pivot table to organize rows by consumer and product
yogl = yogl.pivot_table(index=['id', 'product'], columns='type', values='value', aggfunc='first').reset_index()

#create dummy variables for yog1, yog2, yog3
for j in range(1, 4):
    yogl[f'Yogurt_{j}'] = (yogl['product'] == j).astype(int)

#get maximum value for y1, y2, y3, y4 for each consumer
choice_columns = ['y1', 'y2', 'y3', 'y4']
yog['choice'] = (yog[choice_columns] == 1).idxmax(axis=1).str.extract(r'(\d)').astype(int)
yogl = yogl.merge(yog[['id', 'choice']], on='id')
yogl['chosen'] = (yogl['product'] == yogl['choice']).astype(int)

print(yogl.head())
```

:::: {.callout-note collapse="true"}
### Estimation

_todo: Code up the log-likelihood function._

_todo: Use `optim()` in R or `optimize()` in Python to find the MLEs for the 5 parameters ($\beta_1, \beta_2, \beta_3, \beta_f, \beta_p$).  (Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)_
::::

First I coded up the log-likelyhood function for the wide dataframe then I coded up a log-likelyhood for the long dataframe.

```{python}
#make the feature matrix 
feature_columns = ['Yogurt_1', 'Yogurt_2', 'Yogurt_3', 'f', 'p']

#make X and convert to float
X = yogl[feature_columns].astype(float).values  
```

#### Wide Format
```{python}
#convert data to a single choice index per consumer
def convert_choice_data(yog):
    choice_matrix = yog[['y1', 'y2', 'y3', 'y4']].values

     #index of the chosen product
    choice_indices = np.argmax(choice_matrix, axis=1) 
    return choice_indices

#convert choice data
choice = convert_choice_data(yog) 

#negative log likelyhood
def neg_log_likelihood(beta, X, choice):
    utility = X.dot(beta)
    num_choices = 4
    num_consumers = len(X) // num_choices
    utility = utility.reshape(-1, num_choices)
    exp_utility = np.exp(utility)
    sum_exp_utility = np.sum(exp_utility, axis=1, keepdims=True)
    probability = exp_utility / sum_exp_utility
    log_probability = np.log(probability)
    log_likelihood = np.sum(log_probability[np.arange(num_consumers), choice])
    return -log_likelihood 

beta_initial = np.zeros(X.shape[1]) 
result = minimize(neg_log_likelihood, beta_initial, args=(X, choice), method='BFGS')
```

#### Long Format
```{python}
def neg_log_likelihood(beta, X, choice):
    #get the utility for each product choice
    utility = X.dot(beta)  
    num_choices = 4
    num_consumers = int(len(X) / num_choices) 

    #reshape utility to have one row per consumer and one column per choice
    utility = utility.reshape(num_consumers, num_choices) 

    #get probabilities
    exp_utility = np.exp(utility - np.max(utility, axis=1, keepdims=True))  # For numerical stability
    sum_exp_utility = np.sum(exp_utility, axis=1, keepdims=True)
    probability = exp_utility / sum_exp_utility

    # Calculate log-likelihood
    log_probability = np.log(probability)

    selected_log_prob = log_probability[np.arange(num_consumers), choice.astype(int)]  
    log_likelihood = np.sum(selected_log_prob)

    #-likelyhood
    return -log_likelihood 

beta_initial = np.zeros(X.shape[1])  

#optimize
result = minimize(neg_log_likelihood, beta_initial, args=(X, choice), method='BFGS')
print("Estimated coefficients:", result.x)
```

:::: {.callout-note collapse="true"}
### Discussion

We learn...

_todo: interpret the 3 product intercepts (which yogurt is most preferred?)._

_todo: use the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value._
::::

Based on the wide model and the long model we see what we expected, specifucally there being 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.

 - Yogurt 1 (1.39): The positive coefficient means Yogurt 1 is more likely to be chosen when compared.

- Yogurt 2 (0.64): The positive coefficient means that Yogurt 2 is also more likely to be chosen when compared.

- Yogurt 3 (-3.09): The negative coefficient means that Yogurt 3 is less likely to be chosen when compared.

- Featured Status (0.49):The positive coefficient means that being featured increases the probability of the yogurt being chosen. 

- Price (-37.06): The large negative coefficient for shows that higher prices significantly decrease the probability of the yogurt being chosen which makes sense.

Next I Found the utility difference, price  coefficient and dollar benefit per unit.

```{python}
estimated_coefficients = np.array([1.38775541, 0.64350486, -3.08611796, 0.48741262, -37.05800644])

#get the intercepts and price coefficient from estimated
intercepts = estimated_coefficients[:3]  
price_coefficient = estimated_coefficients[-1]  

#get the most and least preferred yogs
most_preferred_intercept = np.max(intercepts)
least_preferred_intercept = np.min(intercepts)

#utility difference
utility_difference = most_preferred_intercept - least_preferred_intercept

#convert difference to dollar value using the negative price coefficient
dollar_benefit = utility_difference / -price_coefficient 

print (utility_difference, price_coefficient, dollar_benefit)
```

Additionally by using the estimated price coefficient as a dollar-per-util conversion factor we can calculate the dollar benefit between the most-preferred yogurt. 

- Utility Difference (4.47): The difference between the preferred yogurt (Yogurt 1) and the least-preferred yogurt (Yogurt 3) is  4.47.

- Price Coefficient (-37.06): This means for every one increase in price, the utl decreases by 37.06.

- Dollar Benefit per Unit (0.12): This means that the difference between the most preferred and least preferred yogurt of monetary value is about 12 cents per yogurt.


##### TODO: Adjusted Market Share
:::: {.callout-note collapse="true"}
_todo: calculate the market shares in the market at the time the data were collected.  Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of $N \times 4$ estimated choice probabilities.  Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1.  Do the yogurt 1 market shares decrease?_
::::

To calculate the changes that occur if a $0.10 price increase to yogurt A I use the fitted model to predict p(y|x) for each consumer and each product.

```{python}
#extract columns and normalize price
feature_columns = ['Yogurt_1', 'Yogurt_2', 'Yogurt_3', 'f', 'p']
X = yogl[feature_columns].astype(float).values
X[:, -1] = (X[:, -1] - np.mean(X[:, -1])) / np.std(X[:, -1])

#number of entries in X is divisible by num_choices
num_choices = 4
num_consumers = len(X) // num_choices
choice = yogl.loc[yogl['chosen'] == 1, 'product'].astype(int).values - 1  

def neg_log_likelihood(beta, X, choice):
    utility = X.dot(beta)
    utility = utility.reshape(-1, num_choices)
    exp_utility = np.exp(utility - np.max(utility, axis=1, keepdims=True))  # Numerical stability
    sum_exp_utility = np.sum(exp_utility, axis=1, keepdims=True)
    probability = exp_utility / sum_exp_utility
    log_probability = np.log(probability)
    selected_log_prob = log_probability[np.arange(len(choice)), choice]
    log_likelihood = np.sum(selected_log_prob)
    return -log_likelihood  

#initial beta values
beta_initial = np.zeros(X.shape[1])

#optimization
result = minimize(neg_log_likelihood, beta_initial, args=(X, choice), method='L-BFGS-B', options={'maxiter': 10000, 'disp': True})
beta_estimated = result.x

#calculate current market shares
def calculate_market_shares(X, beta):
    utility = X.dot(beta)
    utility = utility.reshape(-1, num_choices)
    exp_utility = np.exp(utility - np.max(utility, axis=1, keepdims=True))
    sum_exp_utility = np.sum(exp_utility, axis=1, keepdims=True)
    probability = exp_utility / sum_exp_utility
    market_shares = np.mean(probability, axis=0)
    return market_shares, probability

current_market_shares, current_probabilities = calculate_market_shares(X, beta_estimated)
print("Current Market Shares:", current_market_shares)

#increase the price of yog1 by $0.10 
X_new = X.copy()
price_increase = 0.10 / np.std(yogl['p'])  
X_new[yogl['product'] == 1, -1] += price_increase

#new market shares 
new_market_shares, new_probabilities = calculate_market_shares(X_new, beta_estimated)
print("New Yogurt 1 market share:", new_market_shares)

#change in market shares
print("Change in Market Shares:", new_market_shares - current_market_shares)
```

Change in Market Shares

- Yogurt 1: Decreased by 32.09%

- Yogurt 2: Increased by 18.99%

- Yogurt 3: Increased by 1.48%

From the model we can see that if the price of yogurt 1 goes up by $0.10 the market share does decrease by a substanticaal 32%. Subsequently this means that the popularity of yogurt 2 and 3 increases by 19% and 1% respectively. This shows that customers demand of yogurt is elastic. 


## 2. Estimating Minivan Preferences

:::: {.callout-note collapse="true"}
### Data

_todo: download the dataset from here:_ http://goo.gl/5xQObB 

_todo: describe the data a bit. How many respondents took the conjoint survey?  How many choice tasks did each respondent complete?  How many alternatives were presented on each choice task? For each alternative._
::::

```{python}
rin = pd.read_csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/rintro-chapter13conjoint.csv")
```

```{python}
rin.head()
```

```{python}
rin.describe()
```

```{python}
rin.info()
```

```{python}
print(rin.columns)
```

Then I found how many respondents took the conjoint survey, how many choice tasks each respondent completed, and lastly how many alternatives were presented on each choice task for each alternative.

```{python}
#num of respondents
num_respondents = rin['resp.id'].nunique()
print(f"Number of respondents: {num_respondents}")

#num of choice tasks per respondent
num_choice_tasks_per_respondent = rin.groupby('resp.id')['ques'].nunique().max()
print(f"Number of choice tasks per respondent: {num_choice_tasks_per_respondent}")

#alternatives for choice task
num_alternatives_per_task = rin.groupby(['resp.id', 'ques']).size().max()
print(f"Number of alternatives per choice task: {num_alternatives_per_task}")

#alternatives
alternatives_summary = rin.groupby(['ques', 'alt']).size().reset_index(name='counts')
print(alternatives_summary)
```

- The number of respondents that took the conjoint survey is 200.
- The number of choice tasks each respondent completed is 15.
- The number of alternatives that were presetned is 3.


:::: {.callout-note collapse="true"}
### TODO: Model
The attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).

_todo: estimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine). Include price as a continuous variable. Show a table of coefficients and standard errors.  You may use your own likelihood function from above, or you may use a function from a package/library to perform the estimation._  
::::

To estimate a MNL model omitting the following levels to avoide multicollinearity I've firsted started with the statsmodels. Unfortunatly I could not get this to work, therefore I decided to use the scikit-learn instead.

I also got created an accuracy metric and a confusion matrix to gauge how accurate this model is.

##### sct learn
```{python}
#reload data
rin = pd.read_csv("C:/Users/kcrah/Desktop/Quarto_stuff/files/rintro-chapter13conjoint.csv")

#convert and clean 
rin['seat'] = pd.to_numeric(rin['seat'], errors='coerce')
rin['cargo'] = pd.to_numeric(rin['cargo'], errors='coerce')
rin['price'] = pd.to_numeric(rin['price'], errors='coerce')
rin['choice'] = pd.to_numeric(rin['choice'], errors='coerce')

rin = rin.assign(
    seat=rin['seat'].fillna(rin['seat'].mean()),
    cargo=rin['cargo'].fillna(rin['cargo'].mean()),
    price=rin['price'].fillna(rin['price'].mean()),
    choice=rin['choice'].fillna(rin['choice'].mode()[0]),
    eng=rin['eng'].fillna('gas')
)

#create dummy vals
rin_dummies = pd.get_dummies(rin, columns=['seat', 'cargo', 'eng'], drop_first=True)

expected_columns = ['seat_7', 'seat_8', 'cargo_3', 'eng_hyb', 'eng_elec', 'price']
for col in expected_columns:
    if col not in rin_dummies.columns:
        rin_dummies[col] = 0

#Define X and Y
X = rin_dummies[expected_columns]
y = rin_dummies['choice']

#standardize 
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#fit the ML Regression model
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)
model.fit(X_scaled, y)

#pred
y_pred = model.predict(X_scaled)

#accuracy 
accuracy = accuracy_score(y, y_pred)
print("\nAccuracy:", accuracy)

#confusionmatrix 
cm = confusion_matrix(y, y_pred)
print("\nConfustion Matrix:",cm)
```

Using that model I got the following results:

##### Coefficients:
- seat_7: -0.1127

- seat_8: -0.0618

- cargo_3: 0.0000

- eng_hyb: -0.0253

- eng_elec: 0.0000

- price: -0.3006

- Intercepts: -0.3795

- Accuracy: (0.68) 

##### Confusion Matrix

- True Negatives: 5562

- False Positives: 438 

- False Negatives: 2427 

- True Positives: 573

From the accuracy matrix we see that the model has 68% accurate.

From the confusion matrix we can see that the model performs well in predicted true negatives but not as well in predicting true positives.


:::: {.callout-note collapse="true"}
### TODO: Coefficeints and Results

_todo: Interpret the coefficients. Which features are more preferred?_

_todo: Use the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?_
::::

##### Coefficients:

- seat_7: -0.1127

- seat_8: -0.0618

- cargo_3: 0.0000

- eng_hyb: -0.0253

- eng_elec: 0.0000

- price: -0.3006

The feature that is most prefered is price as its the most significant factor because of the large negative coefficient. This means like the yogurt example, customers have a very elastic demand for the minivan market.


```{python}
#coefficients
price_coefficient = -0.30063783

#cargo_3 coefficient
cargo3_coefficient = 0.1

#Calculate Dollar Value
dollar_value_cargo3 = cargo3_coefficient / abs(price_coefficient)
dollar_value_cargo3
```

The dollar value of a 3ft cargo space compared to a 2ft cargo space is 0.33.

:::: {.callout-note collapse="true"}
#### TODO: 6 Minivans
_todo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market._

| Minivan | Seats | Cargo | Engine | Price |
|---------|-------|-------|--------|-------|
| A       | 7     | 2     | Hyb    | 30    |
| B       | 6     | 2     | Gas    | 30    |
| C       | 8     | 2     | Gas    | 30    |
| D       | 7     | 3     | Gas    | 40    |
| E       | 6     | 2     | Elec   | 40    |
| F       | 7     | 2     | Hyb    | 35    |


_hint: this example is taken from the "R 4 Marketing Research" book by Chapman and Feit. I believe the same example is present in the companion book titled "Python 4 Marketing Research".  I encourage you to attempt these questions on your own, but if you get stuck or would like to compare you results to "the answers," you may consult the Chapman and Feit books._
::::

To predict the market shares of each minivan in the market I started by creating the dataframe with the given data.

```{python}
#create dataframe
minivans = pd.DataFrame({
    'Seats': [7, 6, 8, 7, 6, 7],
    'Cargo': ['2ft', '2ft', '2ft', '3ft', '2ft', '2ft'],
    'Engine': ['Hyb', 'Gas', 'Gas', 'Gas', 'Elec', 'Hyb'],
    'Price': [30, 30, 30, 40, 40, 35]
})

#categorical vals to dummy vals 
minivans_dummies = pd.get_dummies(minivans, columns=['Seats', 'Cargo', 'Engine'], drop_first=True)

#expected columns from minivans dataframe
expected_columns = ['seat_7', 'seat_8', 'cargo_3', 'eng_hyb', 'eng_elec', 'price']

#account for missings 
for col in expected_columns:
    if col not in minivans_dummies.columns:
        minivans_dummies[col] = 0

#filter and reorder to match training data
minivans_dummies = minivans_dummies[expected_columns]

#standardize features
X_minivans_scaled = scaler.transform(minivans_dummies)

#predict the probabilities
utilities = model.predict_proba(X_minivans_scaled)
market_shares = np.mean(utilities, axis=0)

minivan_names = ['A', 'B', 'C', 'D', 'E', 'F']
market_shares_chosen = market_shares[1]
market_share_df = pd.DataFrame({'Minivan': minivan_names, 'Market Share': [market_shares_chosen] * len(minivan_names)})
print(market_share_df)
```

As we can see the market share...